1. with hints (markers along path)
2. with corrections (changes near marker relative to fixed path)

(3.) with corrections (misspecified reward function)

compare:
  RL from scratch
  predict policy from env then fine-tune
  predict policy from env + {hint, correction} then fine-tune
  predict reward from env then fine-tune
  predict reward from env + {hint, correction} then fine-tune

modeling:
  convolutional board representation
